= Anypoint Monitoring and Visualizer

ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

== Overall Architecture

Anypoint Monitoring is an optional feature during installation, which includes Anypoint Visualizer. Both products require 3 dedicated nodes, normally labeled *amv_node*.

In those 3 nodes, the following Helm charts will be installed in 4 different namespaces:

. `dias`

.. dias-prov-k8s-am-influxdb-comp
.. dias-prov-k8s-am-ingestor-comp
.. dias-prov-k8s-amr-certs-comp
.. dias-prov-k8s-amr-ingestor-router-comp
.. dias-prov-k8s-insight-comp
.. dias-provisioning-api

. `monitoring-center`

.. monitoring-center-alerts-api
.. monitoring-center-metrics-api
.. monitoring-center-settings-api
.. monitoring-center-ui
.. monitoring-center-ui-api
.. monitoring-center-visualizer

. `default`

.. stolon-amv

. `visualizer`
.. visualizer-experience-api
.. visualizer-janitor
.. visualizer-topology-api
.. visualizer-topology-processor
.. visualizer-ui

It's important to mentioned that a dedicated Stolon DB will be deployed in addition to the one used by the platform.

=== DIAS Components

DIAS provides the foundations for data transmission between the Mule runtime (running Filebeat) and the InfluxDB storing that data. Once data reaches InfluxDB, it becomes available to the Anypoint Monitoring UI as well as the Insight alerting engine.

The `dias-prov-k8s-amr-ingestor-router-comp` pods run Nginx accepting connections from remote runtimes thought mTLS (it uses a node-port service). Data is then forwarded to `dias-prov-k8s-am-ingestor-comp` pods running Logstash.

Logstash processes and transform those messages and batch them before hitting InfluxDB. The `dias-prov-k8s-am-influxdb-comp` component is a stateful InfluxDB cluster consisting in 3 meta and 2 data nodes.

The Insight alerting engine is managed by the `dias-prov-k8s-insight-comp` component. It periodically fetches metrics from InfluxDB and based on the customer's alerts definition, the engine triggers alarms. Each pod has 4 containers, including a Mule application, Meld, Meld-Drill and Cassandra.

Logstash, Insight and InfluxDB are all K8s stateful sets.


=== Monitoring Center Components

Mati

=== Visualizer Components

Yevhen



self-signed certs

== Considerations

=== Cluster secrets

All Anypoint Monitoring/Visualizer components use self-signed certificates for TLS support. Those certificates are generated by the `dias-prov-k8s-amr-certs-comp` components and stored as secrets within the cluster under the `dias` namespace. It is important to notice if the customer changes the platform DNS, the `dias-prov-k8s-amr-certs-comp` will be redeployed and secrets will be updated. Additionally, the `dias-prov-k8s-amr-ingestor-router-comp` component, running Nginx, will be redeployed as well, refreshing those new secrets.

=== Stateful sets directories

Each stateful set uses different directories to hold its data, resulting in the following directories:

---
  /var/lib/data/influxdb/data
  /var/lib/data/influxdb/meta
  /var/lib/data/logstash
  /var/lib/data/dias-cassandra
  /var/lib/data/dias-meld

InfluxDB meta pods (3 pods) use `/var/lib/data/influxdb/meta` while data pods (2 pods) use `/var/lib/data/influxdb/data`. Logstash uses `/var/lib/data/logstash` and finally Insight uses the remaining two directories to hold its data.

== Requirements

Hardware requirements are listed in the main PCE hardware section but 3 nodes are required for all Anypoint Monitoring/Visualizer workload.

Port 8895 needs to be accesible in the main balancer along with the rest of the required ports.


=== InfluxDB License

InfluxDB requires a valid license to operate.

== Restore/Backup

Both backup and restore procedures for Anypoint Monitoring are outside the normal Gravitational backup/restore mechanism and therefore requires additional procedures.

Anypoint Monitoring/Visualizer need to backup the following component's data

. InfluxDB (`dias-prov-k8s-am-influxdb-comp`)
. Cassandra ('dias-prov-k8s-am-ingestor-comp')


Each component has its own way to backup, and restore, the data. It is important to mentioned that the Anypoint Monitoring/Visualizer custom Stolon DB will be backup/restore by the default Gravitational mechanism and it is not considered in this document.

---
It is up to the customer to provide enough space to run the backup/restore for each of the mentioned components, considering that we will need double the space, at most. For instance, InfluxDB data directories might use 200GB and therefore the underlying disk (where the backup is meant to be written) should have 200GB+ free space.
---

=== InfluxDB

All InfluxDB data/meta nodes run on the 3 nodes labeled `amv_node` and therefore we need to backup the data sitting in those 3 nodes. Data is under '/var/lib/data/influxdb' for both meta and data pods. The following steps consider that the underlying hard drive has enough space to hold the backup. The destination can be actually anything (disk, S3 bucket, etc).

. `kubectl get nodes | grep amv` will provide the Anypoint nodes
. Connect to each `amv` node and run `sudo gravity enter`
. Under `cd /var/lib/data/influxdb` we can find the data that needs to be backup. At this point, any approach to copy that data to a secure location is acceptable. The following steps are provided just as a local backup.
. Copy over data with `cp -a meta/. bck/meta & cp -a data/. bck/data` (`bck` is just a directory in the same location, it can be anywhere)
. On a fresh new installation with InfluxDB pods already running we will restore data with `cp -a bck/data/. data/ & cp -a bck/meta/. meta/`
. Restarting all meta/data pods will reload that data


Backing up InfluxDB is as simple as copying over all the data under `/var/lib/data/influxdb`. Upon restore, all influxDB pods need to be restarted (if they are already running) to restore the data.

=== Cassandra

Ejaz to complete

=== Logstash

When it comes to Logstash data (`dias-prov-k8s-am-ingestor-comp`) it is important to consider that Logstash keeps "watermark" files which indicate which metric files have been processed. In a fresh new instal (either from scratch or after a restore) there will not be any metric files to process and therefore no need to backup those "watermark" files. This is acceptable since Logstash has a flush mechanism built-in where upon pod graceful termination, all data is flushed to InfluxDB.
